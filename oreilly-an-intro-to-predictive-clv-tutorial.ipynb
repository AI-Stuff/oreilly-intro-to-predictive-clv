{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# An Intro to Predictive Modeling for Customer Lifetime Value (CLV) -- Tutorial Notebook \n",
    "\n",
    "In this notebook, you will be introduced to the workflow necessary to train a Pareto/NBD model (e.g. Schmittlein et al. 1987) on a transactional dataset. An extension to the Pareto/NBD model includes predictions for the monetary value as well (Gamma-Gamma model -- Fader et al. 2004). \n",
    "\n",
    "The Pareto/NBD model is a good introductory probabilistic model to the non-contractual setting with continous purchase opportunity. It's a simple enough model that is easy to train and generally produces good results when the assumptions behind the model are met. It's a good first shot at CLV modeling ! \n",
    "\n",
    "\n",
    "## A few words on the CDNOW Dataset \n",
    "The CDNOW dataset is a very popular dataset used in academic papers addressing CLV models. CDNOW used to be an online retailer of CDs in the 1990's. The dataset in question includes the transactional data of a cohort of customers who have made their first purchase in the first quarter of 1997. All transactions from these customers between their purchase and June 1998 are included. The transactional data was downsampled to contain transactions of 10% of the customers population (2357 customers). \n",
    "\n",
    "The CDNOW dataset is a good example of a non-contractual setting with a continuous purchasing opportunity. It has been used extensively in the CLV literature.\n",
    "\n",
    "\n",
    "## A Few Warnings/Disclaimers : \n",
    "* In this notebook, I favored code simplicity over performance. Some of you may find that operations done on dataframes and in the STAN code could be optimized for performance. That is certainly the case.\n",
    "* The sample log-likelihood of the Pareto/NBD model can be derived analytically and model parameters can be found via standard optimization routines. In principle, one does not have to use STAN to obtain the parameters of the Pareto/NBD model. The purpose of the STAN code is to allow users to potentially extend the model beyond sample log-likelihoods that cannot be expressed analytically and require MCMC evaluation.\n",
    "* The main focus of this notebook is to predict the purchase count. I left it for you as an exercise to implement the gamma-gamma model for the monetary value component, as described in this paper by Fader et al. (2005). I put the solution at the bottom of the notebook.\n",
    "\n",
    "\n",
    "## Requirements \n",
    "This notebook was tested with \n",
    "* ipython version 3.0.0 \n",
    "* Mac OS X El Captian version 10.11.6\n",
    "\n",
    "We recommend a few Gb of RAM (2-3).\n",
    "\n",
    "Please download and place the file `cdnow_transaction_log.csv` in the same directory as this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Training a Pareto/NBD Model on the CDNOW Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Doing the necessary installations. You can install these from your notebook or \n",
    "# from a terminal window if you are prompted for a password. \n",
    "\n",
    "# You may have to do `sudo pip ...` for all of the packages below. \n",
    "\n",
    "!pip install numpy==1.12.0 \n",
    "!pip install pandas==0.19.2\n",
    "!pip install scipy==0.18.1\n",
    "!pip install matplotlib==2.0.0\n",
    "!pip install pickle\n",
    "!pip install cython==0.23.5\n",
    "!sudo pip install pystan==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doing all the necessary imports here \n",
    "\n",
    "import os \n",
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pystan \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime \n",
    "from scipy.stats import gaussian_kde\n",
    "from hashlib import md5\n",
    "%matplotlib inline \n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "# if you are having issues with matplotlib on macosx, I recommend taking a look \n",
    "# at this stakoverflow thread : \n",
    "# http://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python \n",
    "# 1) cd ~/.matplotlib\n",
    "# 2) vim matplotlibrc\n",
    "# 3) add to the file :  backend: TkAgg\n",
    "# 4) restart the ipython kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset into a Pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the cdnow dataset into a Pandas DataFrame\n",
    "\n",
    "transactions = pd.read_csv('cdnow_transaction_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets Explore the dataframe\n",
    "\n",
    "transactions.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has the familiar structure of a transactional dataset. There is a column for customer ID, a transaction date and the amount of the transaction. We have everything we need to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets convert the data field into a datetime object : \n",
    "\n",
    "transactions['date'] = pd.to_datetime(transactions['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Exploration of the Transactional Data \n",
    "\n",
    "Here we're going to do some basic exploration of the dataset. For the readers who are already familiar with the CDNOW dataset, go to the \"Compute the RFM Dataframe\" section directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of transactions \n",
    "\n",
    "transactions.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of customers : \n",
    "\n",
    "print transactions.groupby(['cust']).size().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# date range of the dataset : \n",
    "\n",
    "print transactions['date'].min(), transactions['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset spans 1.5 year of data. A natural breakdown would be to train on one year of data and validate on the following 6 months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Time series of the number of transactions (daily) \n",
    "\n",
    "ts_transactions = transactions.groupby(['date']).size()\n",
    "plt.ylabel('Transaction Count') \n",
    "ts_transactions.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above looks at the number of transactions per day for the 2357 customers in this dataset. As you probably \n",
    "noticed, there is a sharp break in the counts at the end of `1997-03`. That is because the dataset only includes the customers who made their first purchase in the first quarter of 1997. If the dataset would have also included the customers who made their first purchase post 1997-Q1, the count per day would have likely continued to increase. \n",
    "\n",
    "This is a standard practice when modeling CLV. The cohorts of customers used to train the models are generally based on their time of first purchase. That way, one can study the evolution of the population parameters over time and pinpoint possible problems in the long run. \n",
    "\n",
    "The drawback of using such cohort-based approach is that you have fewer datapoints to constrain the priors parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the typical inter-purchase time and IPT distribution \n",
    "\n",
    "def shift_date(x): \n",
    "    x['shifted_date'] = x['date'].shift(-1) \n",
    "    return x\n",
    "\n",
    "# We'll apply a shift of -1 between the 'date' column and a newly shifted date column shift_date. \n",
    "# That way, we'll be able to subtract date from shifted_date at the customer level and compute \n",
    "# the inter purchase time (IPT) directly : \n",
    "transactions_tmp = transactions.sort_values(['date']).\\\n",
    "                        groupby(['cust'], as_index=True).apply(shift_date)    \n",
    "\n",
    "# Let's re-order by customer and date : \n",
    "transactions_tmp.sort_values(['cust','date'], ascending=True, inplace=True)\n",
    "transactions_tmp.dropna(inplace=True)\n",
    "\n",
    "# Compute the IPT in days : \n",
    "transactions_tmp['IPT'] = (transactions_tmp['shifted_date'] - transactions_tmp['date']).apply(lambda x : x.days)\n",
    "\n",
    "transactions_tmp.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's print the mean IPT. Our training period of 365 days is long enough. \n",
    "\n",
    "print(transactions_tmp['IPT'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribution of IPT : \n",
    "\n",
    "transactions_tmp['IPT'].hist(bins=40)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('IPT (days)') \n",
    "plt.ylabel('Number of Purchases') \n",
    "\n",
    "# 275 (365-90) days to avoid right censorship issues. \n",
    "plt.xlim([0,270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the number of purchases per customer : \n",
    "\n",
    "n_purchases = transactions.groupby(['cust']).size()\n",
    "print(n_purchases.min(axis=0), n_purchases.max(axis=0))\n",
    "n_purchases.hist(bins=(n_purchases.max(axis=0) - n_purchases.min(axis=0)) + 1)\n",
    "plt.xlabel('Number of Purchases') \n",
    "plt.ylabel('Number of Customers') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the figure above, more than 50% (1200/2357) of the customers made only a single purchase in the 1.5 year period covered by the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Compute the RFM Dataframe \n",
    "\n",
    "In this section, we will generate the recency-frequency-monetary value (RFM) object. \n",
    "\n",
    "* Recency : time between first and last transaction\n",
    "* Frequency : here frequency really refers to repeat frequency, i.e. the number of purchases beyond the initial one. i.e. repeat frequency = purchase counts - 1 )\n",
    "* monetary value : mean of all the transactions in the training periods \n",
    "* T : time between first purchase and end of calibration period \n",
    "\n",
    "RFM data structures are used to constrain the Pareto/NBD and BG/NBD models. Extensions to the RFM structure are needed for the Pareto/GGG model (Platzer & Reutterer 2016) or to include the clumpiness estimate (Zhang et al. 2015), for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select calibration and holdout periods \n",
    "# Lets select a training period of one year and and a holdout period of 6 months. \n",
    "\n",
    "end_calibration = pd.to_datetime('1997-12-31')\n",
    "train = transactions[transactions.date <= end_calibration]\n",
    "holdout = transactions[transactions.date > end_calibration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Bin transactions by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sum the monetary value by customer and by date : \n",
    "\n",
    "train2 = train.sort_values(['date'], ascending=True).groupby(['cust', 'date'], \n",
    "                                                             as_index=False)['sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the data : \n",
    "\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define a single function that can be applied at the customer level and \n",
    "# compute all the relevant RFM quantities at once : \n",
    "\n",
    "def compute_rfm(x, end_calibration): \n",
    "    x['recency'] = (x['date'].max() - x['date'].min()).days\n",
    "    x['frequency'] = x['date'].count()-1\n",
    "    x['T'] = (end_calibration - x['date'].min()).days\n",
    "    x['monetary_value'] = x['sales'].mean()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the function compute_rfm to compute recency, frequency, T and monetary value \n",
    "# for each group (each customer). \n",
    "\n",
    "train3 = train2.groupby(['cust']).apply(lambda x: compute_rfm(x, end_calibration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the data : \n",
    "\n",
    "train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets take the first row for each customer and only the relevant columns of interest. \n",
    "\n",
    "rfm = train3[['cust', 'recency', 'frequency', 'T', 'monetary_value']].groupby(['cust']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the results : \n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the range of values for each column. Make sure the results make sense \n",
    "# before going any further with the analysis. No NaNs, \n",
    "# no negative values, no recency > 364 days, etc.  \n",
    "\n",
    "rfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look good so far! You could also take a look at the distribution of recency, frequency, T, and monetary value. before going any further. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Train a simple Pareto/NBD Model over the training/holdout period. \n",
    "\n",
    "Now that we have an RFM vector for each customer and for the training period (`rfm` dataframe above), let's train the Pareto/NBD object). For this step, we're going to use the python API to the STAN library. <a href=\"mc-stan.org\">STAN</a> is a probabilistic langauge and has been used in many fields, including physics, engineering, and business. STAN provides APIs for several languages including R, Python, Matlab, and Julia. \n",
    "\n",
    "The script below is divided into 4 parts : \n",
    "\n",
    "* data (the raw data) \n",
    "* parameters (the model parameters that have to be evaluated) \n",
    "* model (code of the model) \n",
    "\n",
    "The individual-level likelihood function of the Pareto/NBD model can be easily derived (e.g. Schmittlein et al. 1987; Fader et al. 2005) and will be used in the STAN code below : \n",
    "$$ L(\\lambda, \\mu | x, t_x, T) = \\frac{\\lambda^x \\mu}{\\lambda+\\mu}e^{-(\\lambda+\\mu)t_x}+\\frac{\\lambda^{x+1}}{\\lambda+\\mu}e^{-(\\lambda+\\mu)T} $$\n",
    "\n",
    "As discussed during the webcast, $\\lambda$ is the count rate that goes in the Poisson distribution and $\\mu$ is the slope of the lifetime exponential distribution. The typical lifetime corresponds to $\\sim 1/\\mu$. \n",
    "\n",
    "The priors for $\\lambda$ and $\\mu$ are gamma distributed : \n",
    "$$g(\\lambda|r,\\alpha) = \\frac{\\alpha^r}{\\Gamma(r)}\\lambda^{r-1}e^{-\\lambda \\alpha} $$\n",
    "and \n",
    "$$g(\\mu|s,\\beta) = \\frac{\\beta^s}{\\Gamma(s)}\\mu^{s-1}e^{-\\mu \\beta} \\; . $$ \n",
    "\n",
    "For each of the four model parameters $(r,\\alpha,s,\\beta)$, I assigned hyperpriors that are normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the STAN model over several 1000's iterations can take a long time. \n",
    "# I recommend running the STAN models for at least 5000 iterations with a warmup \n",
    "# of a few 100. To get a thorough understanding of the code below and the fit results, \n",
    "# I highly recommend reading the STAN documentation. \n",
    "\n",
    "# Although running a STAN model is not necessary to solve a Pareto/NBD model, \n",
    "# it allows you to extend/change the model with ease. \n",
    "\n",
    "paretonbd_model=\"\"\"\n",
    "data{\n",
    "int<lower=0> n_cust; //number of customers \n",
    "vector<lower=0>[n_cust] x; \n",
    "vector<lower=0>[n_cust] tx; \n",
    "vector<lower=0>[n_cust] T; \n",
    "}\n",
    "\n",
    "parameters{\n",
    "// vectors of lambda and mu for each customer. \n",
    "// Here I apply limits between 0 and 1 for each \n",
    "// parameter. A value of lambda or mu > 1.0 is unphysical \n",
    "// since you don't enough time resolution to go less than \n",
    "// 1 time unit. \n",
    "vector <lower=0,upper=1.0>[n_cust] lambda; \n",
    "vector <lower=0,upper=1.0>[n_cust] mu;\n",
    "\n",
    "// parameters of the prior distributions : r, alpha, s, beta. \n",
    "// for both lambda and mu\n",
    "real <lower=0>r;\n",
    "real <lower=0>alpha;\n",
    "real <lower=0>s;\n",
    "real <lower=0>beta;\n",
    "}\n",
    "\n",
    "model{\n",
    "\n",
    "// temporary variables : \n",
    "vector[n_cust] like1; // likelihood\n",
    "vector[n_cust] like2; // likelihood \n",
    "\n",
    "// Establishing hyperpriors on parameters r, alpha, s, and beta. \n",
    "r ~ normal(0.5,0.1);\n",
    "alpha ~ normal(10,1);\n",
    "s ~ normal(0.5,0.1);\n",
    "beta ~ normal(10,1);\n",
    "\n",
    "// Establishing the Prior Distributions for lambda and mu : \n",
    "lambda ~ gamma(r,alpha); \n",
    "mu ~ gamma(s,beta);\n",
    "\n",
    "// The likelihood of the Pareto/NBD model : \n",
    "like1 = x .* log(lambda) + log(mu) - log(mu+lambda) - tx .* (mu+lambda);\n",
    "like2 = (x + 1) .* log(lambda) - log(mu+lambda) - T .* (lambda+mu);\n",
    "\n",
    "// Here we increment the log probability density (target) accordingly \n",
    "target+= log(exp(like1)+exp(like2));\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# here's the data we will provide to STAN : \n",
    "data={'n_cust':len(rfm),\n",
    "    'x':rfm['frequency'].values,\n",
    "    'tx':rfm['recency'].values,\n",
    "    'T':rfm['T'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STAN models can take a while to fit. Let's pickle the model to disk as a precautionary measure. \n",
    "# We can always read the model from disk later on. Note that this file is sizable > 100 MB. \n",
    "\n",
    "# Utility function to pull a stan model that has been pickled. \n",
    "# from pystan docs : https://pystan.readthedocs.io/en/latest/avoiding_recompilation.html\n",
    "def stan_cache(model_code, model_name=None, **kwargs):\n",
    "    \"\"\"Use just as you would `stan`\"\"\"\n",
    "    code_hash = md5(model_code.encode('ascii')).hexdigest()\n",
    "    if model_name is None:\n",
    "        cache_fn = 'cached-model-{}.pkl'.format(code_hash)\n",
    "    else:\n",
    "        cache_fn = 'cached-{}-{}.pkl'.format(model_name, code_hash)\n",
    "    try:\n",
    "        sm = pickle.load(open(cache_fn, 'rb'))\n",
    "    except:\n",
    "        sm = pystan.StanModel(model_code=model_code)\n",
    "        with open(cache_fn, 'wb') as f:\n",
    "            pickle.dump(sm, f)\n",
    "    else:\n",
    "        print(\"Using cached StanModel\")\n",
    "    return sm.sampling(**kwargs)\n",
    "\n",
    "# I recommend training for several 1000's iterations. Here we run the STAN model : \n",
    "pareto_nbd_fit = stan_cache(paretonbd_model, model_name='pareto_nbd_fit', \\\n",
    "                                  data=data, chains=1, iter=1000, warmup=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we'll extract the traces for the lambda and mu parameters. We get the posterior \n",
    "# distribution of these parameters for \"free\" when using STAN. We can then assign the mean \n",
    "# value of lambda and mu for each customer as their expected lambda and mu values. \n",
    "\n",
    "trace = pareto_nbd_fit.extract()\n",
    "lambdas = trace['lambda']\n",
    "mus = trace['mu']\n",
    "# compute the mean at the individual level : \n",
    "mean_mus = np.mean(mus, axis=0)\n",
    "mean_lambdas = np.mean(lambdas, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons Between Model Predictions and Training Set Observations \n",
    "\n",
    "Here we will perform a series of comparisons between the model predictions in the training period vs the observations over the same period of time. \n",
    "\n",
    "The fit above gives us all four model parameters $(r,\\alpha,s,\\beta)$. Furthermore, the MCMC chain also gives us the values of $\\lambda,\\mu$ for each customer. Given $(\\lambda,\\mu)$ it becomes relatively easy to derive the expected number of purchases made by each customer in the period $[T,T+dt)$ : \n",
    "$$ E[Y(dt)~|~\\lambda, \\mu, \\rm{alive~at~T}] = \\frac{\\lambda}{\\mu}-\\frac{\\lambda}{\\mu}e^{-\\mu t } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot of the Purchase Counts : Observations vs Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_train = 365.0 # 12 months \n",
    "training_predictions = mean_lambdas/mean_mus-mean_lambdas/mean_mus*np.exp(-mean_mus*dt_train)\n",
    "rfm['model_train_count'] = training_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_train_count = (rfm['model_train_count'] - rfm['frequency']).apply(lambda x : x*x)\n",
    "rmse_train_count = np.sqrt(rmse_train_count.sum()/len(rfm))\n",
    "print('RMSE =', rmse_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter(dataframe, colx, coly, xlabel='Observed Counts', \n",
    "                 ylabel='Predicted Counts', \n",
    "                 xlim=[0,15], ylim=[0,15], density=True): \n",
    "    \"\"\"This function will plot a scatter plot of colx on the x-axis vs coly on the y-axis. \n",
    "    If you want to add a color to indicate the density of points, set density=True\n",
    "    \n",
    "    Args : \n",
    "        - dataframe (dataframe) : pandas dataframe containing the data of interest \n",
    "        - colx (str) : name of the column you want to put on the x axis \n",
    "        - coly (str) : same but for the y axis \n",
    "        - xlabel (str) : label to put on the x axis \n",
    "        - ylabel (str) : same for y axis \n",
    "        - xlim (list) : defines the range of x values displayed on the chart \n",
    "        - ylim (list) same for the y axis. \n",
    "        - density (bool) : set True to add color to indicate density of point. \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if not density : \n",
    "        plt.scatter(dataframe[colx].values, dataframe[coly].values)\n",
    "    else:\n",
    "        xvals = dataframe[colx].values\n",
    "        yvals = dataframe[coly].values\n",
    "        xy = np.vstack([xvals, yvals])\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        plt.scatter(xvals, yvals, c=z, s=10, edgecolor='')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(np.linspace(xlim[0], xlim[1], 100), \n",
    "             np.linspace(ylim[0], ylim[1], 100), \n",
    "             color='black')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see the scatter plot predicted vs observed purchase counts in the training period. \n",
    "\n",
    "plot_scatter(rfm, 'frequency', 'model_train_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Though this is expected given that this dataset was used to train the model. A more meaningful comparison would be done on a holdout period that the model has not seen yet. That's what we'll do below. \n",
    "\n",
    "I recommend adding to the diagnostics scatter plot above. A few suggestions include : \n",
    "* A chart showing the residuals per observed counts. This may indicate where the model performs poorly. \n",
    "* A cumulative distribution function (CDF) of the total number of purchases. This will be useful to determine whether or not the model can be used to forecast demand for this particular cohort of customers \n",
    "\n",
    "**All of these and more advanced diagnostics are accessible to our <a href=\"https://www.datascience.com/products\">DataScience Cloud Platform customers</a> in our CLV Playbook and Library. Coming soon (03/17)! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Comparisons Between Predictions and the Holdout (validation) Set Observations\n",
    "\n",
    "As discussed below, we will now take a look at the holdout period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions made over the holdout period of 6 months : \n",
    "dt_hold = 180.0 # 6 months \n",
    "holdout_predictions = mean_lambdas/mean_mus - mean_lambdas/mean_mus*np.exp(-mean_mus*dt_hold)\n",
    "rfm['model_holdout_count'] = holdout_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets look at the observed number of transactions during the same time period : \n",
    "# counts per customer per date : \n",
    "holdout_counts = holdout.groupby(['cust', 'date'], as_index=False).size().reset_index()\n",
    "\n",
    "# counts per customer : \n",
    "holdout_counts = holdout_counts.groupby(['cust']).size()\n",
    "\n",
    "# Let's merge with the rfm object. \n",
    "rfm = rfm.merge(pd.DataFrame(holdout_counts), how='left', left_index=True, right_index=True)\n",
    "rfm.rename(columns={0:'obs_holdout_count'}, inplace=True)\n",
    "rfm.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's now plot the data : \n",
    "\n",
    "rmse_holdout_count=(rfm['model_holdout_count']-rfm['obs_holdout_count']).apply(lambda x :x*x)\n",
    "rmse_holdout_count=np.sqrt(rmse_holdout_count.sum()/len(rfm))\n",
    "print('RMSE =',rmse_holdout_count)\n",
    "plot_scatter(rfm, 'obs_holdout_count', 'model_holdout_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly we're not doing as well on the holdout set than we did on the training set. Not too bad though as the RMSE is less than 1 purchase count. \n",
    "\n",
    "There are several ways to improve the holdout results. Segmentation is one of them. \n",
    "\n",
    "**<a href=\"http://www.datascience.com\">DataScience</a>'s advisory services for our platform customers will guide you through the process of implementing and refining your CLV models and put them in production. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Training a simple Gamma-Gamma model on the monetary value \n",
    "\n",
    "The next steps is to look at the monetary value model. This model follows closely the Gamma-Gamma model discussed in Fader et al. (2004). Note that the monetary value component is generally very difficult to model. Many factors can affect the price of items in ways that are not accounted for by the model. Long term changes in prices over several years (akin to inflation), discounts, promotions, etc. are difficult to capture in this and other simple monetary value models. That is something to keep in mind when doing comparisons over the holdout period and making predictions for future purchases. \n",
    "\n",
    "In the gamma model, the observed average order value in the training period is an imperfect metric of the latent mean transaction value $E(M)$ at the customer level. \n",
    "\n",
    "The main assumption behind the gamma model is that the average order value at the customer level is distributed according to a gamma distribution of shape $p$ and scale $\\nu$ \n",
    "$$ p(m_x~|~p, \\nu, x) = \\frac{(\\nu x)^{px}m_x^{px-1}e^{-\\nu x m_x}}{\\Gamma(px)}$$\n",
    "\n",
    "where \n",
    "* $x$ is the total number of transactions (`rfm['frequency']+1`) \n",
    "* $m_x$ is the average order value\n",
    "* $p$ is the shape of the gamma distribution. The model assumes that this parameter is the same for all customers. \n",
    "* $\\nu$ is the scale parameter. $\\nu$ varies across customers and has a prior that is also gamma distributed with parameters $(q,\\gamma)$\n",
    "\n",
    "Ultimately, this model will give us the expected average transaction value for a customer with an average spend of $m_x$ dolalrs across $x$ transactions in the training period: \n",
    "$$E(M~|~p,q,\\gamma,m_x,x) = \\frac{(\\gamma + m_x x)p}{px+q-1} $$\n",
    "\n",
    "$E(M~|~p,q,\\gamma,m_x,x)$ will then be multiplied by $E[Y(dt)~|~\\lambda, \\mu, \\rm{alive~at~T}]$ to give us the CLV of each customer in the holdout period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This gamma-gamm model follows the Fader et al. (2004) Gamma-Gamma model closely. \n",
    "# Again, this model can take a while to train. Recommend a few 1000's iterations. \n",
    "\n",
    "gamma_gamma_model=\"\"\"\n",
    "data {\n",
    "    // this is the data we pass to STAN : \n",
    "    int<lower=1> n_cust;         // number of customers \n",
    "    vector[n_cust] x;            // frequency + 1 \n",
    "    vector[n_cust] mx;           // average purchase amount for each customer \n",
    "}\n",
    "\n",
    "parameters {\n",
    "    // These are the model parameters : \n",
    "    real <lower=0>p;             // scale parameter of the gamma distribution. Note that \n",
    "                                 // this parameter is not a vector. All customers will have the \n",
    "                                 // same value of p. \n",
    "    vector<lower=0> [n_cust] v;   // shape parameter of the gamma distribution (nu)\n",
    "    real <lower=0>q;             // shape parameter of the gamma prior distribtion on v \n",
    "    real <lower=0>y;             // scale parameter of the gamma prior distribution on v \n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    vector<lower=0> [n_cust] px;\n",
    "    vector<lower=0> [n_cust] nx; \n",
    "    px <- p * x;                 // getting px from p and x \n",
    "    for (i in 1:n_cust) \n",
    "        nx[i] <- v[i] * x[i]; \n",
    "}\n",
    "\n",
    "model {\n",
    "    p ~ exponential(0.1);    // prior distribution on p\n",
    "    q ~ exponential(0.5);    // hyperprior distribution on q \n",
    "    y ~ exponential(0.1);    // hyperprior distribution on y \n",
    "//    v ~ gamma(q, q ./ y);    // prior distribution on nu  \n",
    "//    mx ~ gamma(px, v);       // likelihood function \n",
    "    v ~ gamma(q,y); \n",
    "    mx ~ gamma(px,nx); \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# here's the data we will provide to STAN : \n",
    "data_gg={'n_cust':len(rfm),\n",
    "    'x':rfm['frequency'].values+1.0,\n",
    "    'mx':rfm['monetary_value'].values\n",
    "     }\n",
    "\n",
    "# I recommend training for several 1000's iterations. \n",
    "gamma_gamma_fit = stan_cache(gamma_gamma_model, model_name='gamma_gamma_fit', \\\n",
    "                                  data=data_gg, chains=1, iter=1000, warmup=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I extract the useful parameters from the fit \n",
    "\n",
    "trace_gg = gamma_gamma_fit.extract()\n",
    "p=np.mean(trace_gg['p'])\n",
    "gamma=np.mean(trace_gg['y'])\n",
    "q=np.mean(trace_gg['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's compute E(M) and join it to the rfm object : \n",
    "\n",
    "E_M = (gamma+rfm['monetary_value']*(rfm['frequency']+1))*p/(p*(rfm['frequency']+1)+q-1.0)\n",
    "rfm = rfm.merge(pd.DataFrame(E_M), how='left', left_index=True, right_index=True)\n",
    "rfm.rename(columns={0:'E_M'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfm[['monetary_value', 'E_M']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons between E(M) and observed mean in training period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's explore the results : \n",
    "\n",
    "plot_scatter(rfm,'monetary_value','E_M', \n",
    "             xlabel='Average Order Value in Training Period ($)', \n",
    "             ylabel='E(M) ($)', \n",
    "             xlim=[0,50], ylim=[0,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, what can be perceived as different \"lines\" correspond to different values of \"x\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons between E(M) and observed mean in holdout/validation period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compute the observed mean transaction value per customer in the holdout period : \n",
    "\n",
    "holdout_value = holdout.groupby(['cust', 'date'], as_index=False)['sales'].sum().reset_index()\n",
    "holdout_value = holdout_value[['cust', 'sales']].groupby(['cust'])['sales'].mean()\n",
    "holdout_value=pd.DataFrame(holdout_value)\n",
    "holdout_value.rename(columns={'sales':'obs_holdout_monetary_value'}, inplace=True)\n",
    "holdout_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge with rfm object : \n",
    "rfm = rfm.merge(holdout_value, how='left', left_index=True, right_index=True)\n",
    "rfm.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(rfm,'obs_holdout_monetary_value','E_M', \n",
    "             xlabel='Average Order Value in holdout Period ($)', \n",
    "             ylabel='E(M) ($)', \n",
    "             xlim=[0,80], ylim=[0,80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart above highlights how difficult it is to accurately model the monetary value. Most of the data points are found along the observed value of 0. This is because most customers did not make a purchase in the holdout period. For the ones who did, the scatter is very large. The model tends to overestimate the monetary value in the holdout period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, computing the CLV in the holdout period and comparing with with model predictions \n",
    "\n",
    "We are at the end of this exercise. The last step is to compute the customer-level CLV predictions for the holdout period. I invite you to do the same comparison for the training period. \n",
    "\n",
    "CLV is obtained by \n",
    "$$CLV(t,t+dt) = E(M~|~p,q,\\gamma,m_x,x)~\\times~ E[Y(dt)~|~\\lambda, \\mu, \\rm{alive~at~T}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute both modeled and observed CLV in the holdout period : \n",
    "\n",
    "rfm['model_holdout_clv'] = rfm['model_holdout_count'] * rfm['E_M']\n",
    "rfm['obs_holdout_clv'] = rfm['obs_holdout_count'] * rfm['obs_holdout_monetary_value']\n",
    "rmse_holdout_clv = (rfm['model_holdout_clv'] - rfm['obs_holdout_clv'])* \\\n",
    "                   (rfm['model_holdout_clv'] - rfm['obs_holdout_clv'])\n",
    "rmse_holdout_clv = np.sqrt(rmse_holdout_clv.sum()/len(rfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the final results : \n",
    "print('RMSE =', rmse_holdout_clv)\n",
    "plot_scatter(rfm, 'obs_holdout_clv', 'model_holdout_clv',\n",
    "             xlabel='Observed Value in the Holdout Period',\n",
    "             ylabel='Modeled Value in the Holdout Period', \n",
    "             xlim=[0,300.0],ylim=[0,300.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In conclusion \n",
    "\n",
    "The Pareto/NBD model is a good introduction to CLV modeling. The gamma-gamma model for monetary value is easy and simple to train. \n",
    "\n",
    "There are however several limitations to these approaches. More sophisticated models could include user segmentation. Bayes hierarchical models may also be able to better discriminate groups of customers who exhibit different behaviors and model their $\\lambda$ and $\\mu$ accordingly. \n",
    "\n",
    "The team of data scientists at DataScience can help you designing and improve these models. For our DataScience Cloud Platform customers, we provide a CLV playbook along with a library of CLV models, diagnostics and support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 679
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
